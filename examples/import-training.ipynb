{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, install the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-documentintelligence==1.0.0b1 azure-search-documents==11.4.0 unidecode==1.3.8 nltk==3.8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus, install NLTK data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import download\n",
    "\n",
    "download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, initialize the clients to Document Intelligence and AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.documentintelligence.aio import DocumentIntelligenceClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "from azure.search.documents import SearchClient\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from random import randint\n",
    "from typing import Dict, Tuple\n",
    "from unidecode import unidecode\n",
    "import asyncio\n",
    "import glob\n",
    "import os.path\n",
    "import re\n",
    "\n",
    "doc_endpoint = \"https://call-center-ai.cognitiveservices.azure.com\"\n",
    "doc_credential = AzureKeyCredential(\"xxx\")\n",
    "doc_client = DocumentIntelligenceClient(\n",
    "    endpoint=doc_endpoint,\n",
    "    credential=doc_credential,\n",
    ")\n",
    "\n",
    "search_endpoint = \"https://call-center-ai.search.windows.net\"\n",
    "search_credential = AzureKeyCredential(\"xxx\")\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_endpoint,\n",
    "    index_name=\"trainings\",\n",
    "    credential=search_credential,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, transform PDF in Markdown text. We are using Document Intelligence for that.\n",
    "\n",
    "Be warned that this step can take a few minutes, depending on the size of the PDF. From a minute for a few pages to 20 minutes for a 1000 pages PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def pdf_to_markdown(source: str) -> Tuple[str, str]:\n",
    "    if os.path.exists(source + \".md\"):  # Test cache\n",
    "        print(f\"Skipping {source}, cached found\")\n",
    "        with open(source + \".md\", \"r\") as file:\n",
    "            return source, file.read()\n",
    "\n",
    "    with open(source, \"rb\") as file:  # Load file content\n",
    "        print(f\"Starting {source}, no cache found\")\n",
    "        await asyncio.sleep(randint(0, 5))  # Avoid API rate limit\n",
    "        doc_poller = await doc_client.begin_analyze_document(\n",
    "            analyze_request=file,\n",
    "            content_type=\"application/octet-stream\",\n",
    "            locale=\"fr-FR\",  # We only have French documents in this dataset\n",
    "            model_id=\"prebuilt-layout\",\n",
    "            output_content_format=\"markdown\",\n",
    "        )\n",
    "        doc_result = await doc_poller.result()\n",
    "\n",
    "        with open(source + \".md\", \"w\") as file:  # Store result in cache\n",
    "            file.write(doc_result.content)\n",
    "\n",
    "        return source, doc_result.content\n",
    "\n",
    "\n",
    "doc_results: Dict[str, str] = {}\n",
    "doc_tasks = []\n",
    "\n",
    "for source in glob.glob(\"dataset/*.pdf\"):\n",
    "    doc_tasks.append(asyncio.create_task(pdf_to_markdown(source)))\n",
    "\n",
    "print(\"Waiting for results...\")\n",
    "for doc_task in asyncio.as_completed(doc_tasks):\n",
    "    source = None\n",
    "    try:\n",
    "        source, content = await doc_task\n",
    "        print(f\"Ended {source}\")\n",
    "        doc_results[source] = content\n",
    "    except HttpResponseError as e:\n",
    "        print(f\"Failed {source}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the Markdown text into smaller blocks, we'll call chuncks. Each block content is minified with a stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': \"# Groupama ASSET MANAGEMENT docu d'inform cle === object le présent docu contient de inform essentiel sur le produit d'invest .il ne s'ag pas d'un docu à caracter commercial .\", 'id': 'dataset_be463817baf2441f9f5193fa10045711_msdoc_pdf-0', 'source_uri': 'dataset/be463817baf2441f9f5193fa10045711_msdoc.pdf', 'title': 'groupama asset management'}, {'content': \"# Groupama ASSET MANAGEMENT ce inform vous sont fourn conform à une oblig légal , afin de vous aid à comprendr en quoi consist ce produit et quel risqu , coût , gain et pert potentiel y sont associ , et de vous aid à le compar à d'autr produit .\", 'id': 'dataset_be463817baf2441f9f5193fa10045711_msdoc_pdf-1', 'source_uri': 'dataset/be463817baf2441f9f5193fa10045711_msdoc.pdf', 'title': 'groupama asset management'}, {'content': \"# GROUPAMA ULTRA SHORT TERM BOND Part E1 (C - EUR) Code ISIN : FR001400JH30 societ de gestion : sit internet : groupam asset manag http : //www .groupama-am .com/fr/ appel le 01 44 56 76 76 pour de plus ample inform .l'autor de march financi ( amf ) est charg du contrôl de groupam asset manag en ce qui concern ce docu d'inform clé .\", 'id': 'dataset_be463817baf2441f9f5193fa10045711_msdoc_pdf-2', 'source_uri': 'dataset/be463817baf2441f9f5193fa10045711_msdoc.pdf', 'title': 'groupama ultra short term bond part e1 c eur code isin fr001400jh30'}, {'content': \"# GROUPAMA ULTRA SHORT TERM BOND Part E1 (C - EUR) Code ISIN : FR001400JH30 groupam asset manag est agré en franc sous le nºgp9302 et réglement par l'amf .dat de derni révis du docu d'inform clé : 29 sept .\", 'id': 'dataset_be463817baf2441f9f5193fa10045711_msdoc_pdf-3', 'source_uri': 'dataset/be463817baf2441f9f5193fa10045711_msdoc.pdf', 'title': 'groupama ultra short term bond part e1 c eur code isin fr001400jh30'}, {'content': '# EN QUOI CONSISTE CE PRODUIT ? typ : organ de plac collect en valeur mobili ( opcvm ) , constitu sous form de fond commun de plac ( fcp ) de droit franc .dur : le produit a été initial cré pour une dur de 99 an .groupam asset manag a le droit de proced à la dissolu du produit de mani unilatéral .', 'id': 'dataset_be463817baf2441f9f5193fa10045711_msdoc_pdf-4', 'source_uri': 'dataset/be463817baf2441f9f5193fa10045711_msdoc.pdf', 'title': 'en quoi consiste ce produit'}]\n",
      "Created 48111 chunks\n"
     ]
    }
   ],
   "source": [
    "lang = \"french\"\n",
    "stemmer = SnowballStemmer(lang)\n",
    "sentence_r = r\"[^\\w\\s+\\-/'\\\",:;()@=]\"\n",
    "\n",
    "\n",
    "def compress_and_clean(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Compress and clean a text.\n",
    "\n",
    "    Use the Snowball stemmer to stem words and remove all special characters, as the LLM does not give a damn.\n",
    "    \"\"\"\n",
    "    text = text.replace(\"\\\\\", \"\")  # Remove all backslashes\n",
    "    text = re.sub(r\":[a-z]*:\", \"\", text)  # Remove all :unselected: and :selected: tags\n",
    "    text = re.sub(r\"<!--[^<>]*-->\", \"\", text)  # Remove all comments\n",
    "    tokenized = word_tokenize(text, language=lang)\n",
    "    tokens = [stemmer.stem(token) for token in tokenized]\n",
    "    prompt = \" \".join(tokens)\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def data(content: str, source_uri: str, title: str, iterator: int) -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Generate a data object for the search index.\n",
    "\n",
    "    Use deterministic ID to avoid duplicates after a new run. Remove all special characters from title.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"content\": content,\n",
    "        \"id\": f\"{'_'.join(re.sub('[^a-z0-9]', ' ', unidecode(source_uri).lower()).split())}-{iterator}\",\n",
    "        \"source_uri\": unidecode(source_uri).lower(),\n",
    "        \"title\": \" \".join(re.sub(\"[^a-z0-9]\", \" \", unidecode(title).lower()).split()),\n",
    "    }\n",
    "\n",
    "\n",
    "chuncks = []\n",
    "iterator = 0\n",
    "\n",
    "for source, content in doc_results.items():\n",
    "    # Skip empty documents\n",
    "    if not content:\n",
    "        continue\n",
    "\n",
    "    # Split the document into sections\n",
    "    for section in content.split(\"\\n#\"):\n",
    "        lines = section.split(\"\\n\")\n",
    "        title = lines[0].strip()\n",
    "        paragraph = \" \".join(lines[1:])\n",
    "\n",
    "        # Skip empty sections\n",
    "        if not section:\n",
    "            continue\n",
    "\n",
    "        # Split the paragraph into sentences and group them into groups of 300 characters\n",
    "        groups = []\n",
    "        group = \"\"\n",
    "        separators = re.findall(sentence_r, paragraph)\n",
    "        splits = re.split(sentence_r, paragraph)\n",
    "        for i, separator in enumerate(separators):\n",
    "            sentence = compress_and_clean(splits[i] + separator)\n",
    "            if len(group) + len(sentence) >= 300:\n",
    "                groups.append(group)\n",
    "                group = \"\"\n",
    "            group += sentence\n",
    "        if group:\n",
    "            groups.append(group)\n",
    "\n",
    "        # Create a data object for each group\n",
    "        for group in groups:\n",
    "            chuncks.append(\n",
    "                data(\n",
    "                    f\"# {title} {group}\",\n",
    "                    source,\n",
    "                    title,\n",
    "                    iterator,\n",
    "                )\n",
    "            )\n",
    "            iterator += 1\n",
    "\n",
    "print(chuncks[:5])\n",
    "print(f\"Created {len(chuncks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, upload the chuncks to AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_size = 10000\n",
    "import_part = chuncks[:import_size]\n",
    "while import_part:\n",
    "    print(f\"Uploading {len(import_part)} documents to Azure Search\")\n",
    "    search_client.merge_or_upload_documents(import_part)\n",
    "    chuncks = chuncks[import_size:]\n",
    "    import_part = chuncks[:import_size]\n",
    "\n",
    "print(f\"There are {search_client.get_document_count()} documents in the index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations! 😎**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus: Clean up the documents from the AI Search index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    docs = search_client.search(search_text=\"*\", select=[\"id\"])\n",
    "    ids = [{\"id\": doc[\"id\"]} for doc in docs if doc[\"id\"]]\n",
    "    if not ids:\n",
    "        break\n",
    "    print(ids[:5])\n",
    "    search_client.delete_documents(ids)\n",
    "    print(f\"Deleted {len(ids)} documents\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
